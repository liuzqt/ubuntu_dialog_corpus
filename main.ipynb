{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "\n",
    "`preprocess.py`\n",
    "combine all dialogs into one text file, one dialog one line, retaining conversation only, removing punctuation, removing stopword\n",
    "\n",
    "`word_count.py`\n",
    "do the word count\n",
    "\n",
    "## notes in preprocessing\n",
    "### stop word\n",
    "I don't have a good stopword resource regarding system dialog, so I first do the word count with common stopword list, then manually add some extra sotp words\n",
    "\n",
    "btw, filtering out stop word might be time consuming using Regex, a good tool for this is [flashtext](https://github.com/vi3k6i5/flashtext), a word matching model based on prefix-tree. Its performance remain O(n), where n is the length of sentence, while Regex might go up to O(mn) where m is the stopword size, which means the flashtext is performance-insentitive regarding the size of keyword set. It reduce my processing time significantly. \n",
    "### stemming\n",
    "Stemming means combining similar word into one. For example, **install** and **installs**\n",
    "\n",
    "Here I use the Stemmer from [NLTK](http://www.nltk.org/)\n",
    "But I found that the stemmer might trim the word at some wrong position. But it's fined to construct word-vector space.\n",
    "\n",
    "### tokenize\n",
    "basically you can write a simple Regex to do this, or even string.split() will do. But I to gain better performance, I use tokenizer from NLTK too. The regex I use for tkenizer is **r'\\w\\S+\\w'**, which retain a word start with character and end with character, this can filter out punctuation, but keep sth. like **don't**  or **pre-installed**, where punctuation appear inside a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load preprocess.py\n",
    "\n",
    "'''\n",
    "\n",
    "@author: ZiqiLiu\n",
    "\n",
    "\n",
    "@file: preprocess.py\n",
    "\n",
    "@time: 2017/11/11 上午1:56\n",
    "\n",
    "@desc: combine all dialogs into one text file, retaining dialogs only\n",
    "'''\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from flashtext import KeywordProcessor\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n",
    "files = glob('./dialogs/**/*.tsv')\n",
    "\n",
    "\n",
    "# pattern1 = re.compile(r'[1-9\\\\.]+')\n",
    "# pattern2 = re.compile(r'.+//.+')\n",
    "# pattern3 = re.compile(r'\\w+')\n",
    "#\n",
    "#\n",
    "# def filter1(s):\n",
    "#     return pattern1.match(s) and pattern2.match(s) and pattern3.match(s)\n",
    "\n",
    "\n",
    "\n",
    "def processing(i, file_list):\n",
    "    tk = RegexpTokenizer(r'\\w\\S+\\w')\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    stopword_processor = KeywordProcessor()\n",
    "    for w in en_stop:\n",
    "        stopword_processor.add_keyword(w, ' ')\n",
    "\n",
    "    with open('stopword_processor.pkl', 'wb') as f:\n",
    "        pickle.dump(stopword_processor, f)\n",
    "\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    with codecs.open('whole_dialogs_stem_%d' % i, 'w', 'utf-8') as out:\n",
    "        for fi in tqdm(file_list):\n",
    "            with codecs.open(fi, 'r', 'utf-8') as f:\n",
    "                sentences = [stopword_processor.replace_keywords(\n",
    "                    line.strip().split('\\t')[-1].lower()) for\n",
    "                    line in f]\n",
    "                words = functools.reduce(lambda x, y: x + y,\n",
    "                                         map(tk.tokenize, sentences))\n",
    "                words = map(p_stemmer.stem, words)\n",
    "                out.write(' '.join(words) + '\\n')\n",
    "\n",
    "\n",
    "def div_list(l, n):\n",
    "    length = len(l)\n",
    "    t = length // n\n",
    "    quaters = [t * i for i in range(0, n)]\n",
    "    ran = range(0, n - 1)\n",
    "    result = [l[quaters[i]:quaters[i + 1]] for i in ran]\n",
    "    result.append(l[quaters[n - 1]:len(l)])\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_num = 8\n",
    "    p = Pool()\n",
    "    div_files = div_list(files, process_num)\n",
    "    for i in range(process_num):\n",
    "        p.apply_async(processing, args=(\n",
    "            i, div_files[i]))\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    output_list = glob('./whole_dialogs_stem_*')\n",
    "    for output in output_list:\n",
    "        os.system('cat %s >> whole_dialogs_stem' % output)\n",
    "        os.system('rm %s' % output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load word_count.py\n",
    "\n",
    "'''\n",
    "\n",
    "@author: ZiqiLiu\n",
    "\n",
    "\n",
    "@file: word_count.py\n",
    "\n",
    "@time: 2017/11/11 上午2:58\n",
    "\n",
    "@desc:\n",
    "'''\n",
    "\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from flashtext import KeywordProcessor\n",
    "from my_prefix_tree import PrefixTree\n",
    "\n",
    "with open('stopword_for_wc', 'r') as f:\n",
    "    stop_word_addon = [line.strip() for line in f]\n",
    "\n",
    "split_pattern = re.compile(r'\\s+')\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "with codecs.open('whole_dialogs', 'r', 'utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        word_list = split_pattern.split(line)\n",
    "        word_counter.update(word_list)\n",
    "for stop_w in stop_word_addon:\n",
    "    del word_counter[stop_w]\n",
    "top_words = sorted(word_counter.items(), key=lambda a: a[1], reverse=True)[:5000]\n",
    "\n",
    "with codecs.open('wc', 'w', 'utf-8') as out:\n",
    "    for key, value in top_words:\n",
    "        out.write('%s\\t%d\\n' % (key, value))\n",
    "\n",
    "keyword_processer = PrefixTree()\n",
    "for w in top_words:\n",
    "    keyword_processer.update(w[0])\n",
    "\n",
    "with open('keyword_processor.pkl', 'wb') as f:\n",
    "    pickle.dump(keyword_processer, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, let's have a look at top 100 words (doesn't use stemmer here because might result in wrongly truncated word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-11T20:16:20.041744Z",
     "start_time": "2017-11-11T20:16:19.920007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu\t1043670\r\n",
      "install\t769511\r\n",
      "sudo\t421245\r\n",
      "file\t388421\r\n",
      "windows\t373818\r\n",
      "installed\t339438\r\n",
      "linux\t312921\r\n",
      "make\t268000\r\n",
      "boot\t258460\r\n",
      "system\t245784\r\n",
      "apt-get\t227627\r\n",
      "server\t209155\r\n",
      "files\t204674\r\n",
      "package\t199069\r\n",
      "partition\t192931\r\n",
      "drive\t182006\r\n",
      "terminal\t174528\r\n",
      "version\t173138\r\n",
      "root\t164448\r\n",
      "desktop\t157562\r\n",
      "kernel\t156742\r\n",
      "gnome\t154771\r\n",
      "driver\t153281\r\n",
      "mount\t139047\r\n",
      "drivers\t136361\r\n",
      "network\t126579\r\n",
      "usb\t120563\r\n",
      "update\t114767\r\n",
      "remove\t111887\r\n",
      "packages\t106975\r\n",
      "installing\t105368\r\n",
      "download\t102518\r\n",
      "firefox\t101489\r\n",
      "nvidia\t100199\r\n",
      "directory\t99513\r\n",
      "upgrade\t99092\r\n",
      "disk\t97785\r\n",
      "manager\t96632\r\n",
      "channel\t95727\r\n",
      "wireless\t94553\r\n",
      "option\t91186\r\n",
      "software\t88775\r\n",
      "password\t87301\r\n",
      "video\t86580\r\n",
      "folder\t84099\r\n",
      "etc\t83512\r\n",
      "synaptic\t83296\r\n",
      "link\t82586\r\n",
      "menu\t81315\r\n",
      "device\t80534\r\n",
      "list\t80483\r\n",
      "connect\t75136\r\n",
      "flash\t75055\r\n",
      "source\t74444\r\n",
      "ssh\t73894\r\n",
      "gui\t73731\r\n",
      "google\t70793\r\n",
      "login\t69963\r\n",
      "kde\t68881\r\n",
      "settings\t68830\r\n",
      "window\t68124\r\n",
      "internet\t65119\r\n",
      "reboot\t64409\r\n",
      "log\t63608\r\n",
      "iso\t61454\r\n",
      "setup\t61123\r\n",
      "mode\t61021\r\n",
      "connection\t60183\r\n",
      "reinstall\t59991\r\n",
      "debian\t59383\r\n",
      "ram\t59292\r\n",
      "ati\t58391\r\n",
      "configure\t58297\r\n",
      "pastebin\t57037\r\n",
      "compiz\t56106\r\n",
      "router\t55062\r\n",
      "installation\t54230\r\n",
      "ntfs\t54107\r\n",
      "partitions\t52838\r\n",
      "config\t52494\r\n",
      "options\t51930\r\n",
      "users\t49288\r\n",
      "client\t49029\r\n",
      "grep\t48978\r\n",
      "port\t47854\r\n",
      "java\t47469\r\n",
      "dpkg\t46330\r\n",
      "deb\t45099\r\n",
      "irc\t44150\r\n",
      "repos\t43510\r\n",
      "mounted\t42754\r\n",
      "xorg.conf\t41715\r\n",
      "bios\t41045\r\n",
      "graphics\t40929\r\n",
      "ive\t40907\r\n",
      "audio\t40640\r\n",
      "nautilus\t40253\r\n",
      "compile\t39984\r\n",
      "mouse\t39853\r\n",
      "intel\t39680\r\n"
     ]
    }
   ],
   "source": [
    "!cat wc | head -n100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic detection\n",
    "here I come up with two possible solution.\n",
    "\n",
    "### 1. Naive pick the key words in the dialog. \n",
    "\n",
    "The hard thing is it's sometimes diffucult to distinguish stopword and keyword. By looking at frequency might get a little help, as most stopword apprear in high frequency. But keyword like 'linux' or 'windows' also have high frequency. So generally speaking, if we don't have a good 'keyword list' and 'stop word list', it's hard to do so. And the keyword list and stopword list might need to be somehow manually constructed, or using more advanced NLP model.\n",
    "\n",
    "The approach I use here, is that I manually pick up hundreds of keywords with highest frequency, and simply extract them from dialogs.\n",
    "\n",
    "I manually build a prefix-tree based on word count. It allow partial match: for example, **ubuntu-server** can match **ubuntu** \n",
    "\n",
    "\n",
    "\n",
    "### 2. LDA (Latent Dirichlet allocation)\n",
    "\n",
    "Basically it's a unsupervised learning model based on crazy statistic. It assumes a article is generated from a mix of  hidden topics, and each model contains a potential words that can be drawn from.\n",
    "\n",
    "So literally we have two distribution, the topic model distribution p(topic|θ) and a word distrubition given a topic model p(word|topic). Notice that θ is a hyper-pamameter\n",
    "\n",
    "Then to generate a article, the process is here:\n",
    "\n",
    "* Chooseparameter θ ～ p(θ); \n",
    "* For each of the N words:   \n",
    "* Choose a topic ～ p(topic|θ); \n",
    "* Choose a word ～ p(word|topic);\n",
    "                \n",
    "                \n",
    "And to classify the topic given an article, we choose the topic that has the highest probability to generate this article. It's similar to maximum posterior probability.\n",
    "\n",
    "But the problem here is it's a unsupervised learning, and actually we don't have a 'keyword' for a topic, what we use to represent the feature of a topic is a word-vector, which is a distribution of word given this topic. Of course we can choose some high frequency words as the key word in this model. \n",
    "\n",
    "And another problem is that, it's likely that given a dialog, we might suggest a keyword not in the article! This might look wried.\n",
    "\n",
    "But on the other hand, this is a good thing, as this model consider the hidden semantic structure. For example, a simple dialog ** Hey, I failed to use 'sudo apt install python3'**. A keyword **python3** can be easily detected, but a human being with some linux knowledge should be able to tell it's a ubuntu-like distribution. And the LDA model might be able to do this.\n",
    "\n",
    "And here is the possible situation:\n",
    "\n",
    "User input: ** Hey, I failed to use 'sudo apt install python3'**\n",
    "\n",
    "Response: your problem might be relative to this topics: **python3, ubuntu, apt, install**\n",
    "\n",
    "And the word **'ubuntu'** looks reasonable even if it doesn't appear in the dialog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's tried the naive model first\n",
    "\n",
    "usage:\n",
    "`python3 predict_naive.py <*.tsv>..`\n",
    "following arguments can be any number of ubuntu dialog in raw tsv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++\r\n",
      "file:\t./test/100001.tsv\r\n",
      "------------------------------\r\n",
      "raw dialog:\r\n",
      "2008-09-02T04:02:00.000Z\tchuck\tbrettley\tHAI THAR\r\n",
      "2008-09-02T04:04:00.000Z\tbrettley\tchuck\tgo awai\r\n",
      "2008-09-02T04:10:00.000Z\tchuck\tbrettley\thello\r\n",
      "\r\n",
      "\r\n",
      "------------------------------\r\n",
      "keywords:\r\n",
      "[]\r\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\r\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\r\n",
      "file:\t./test/100002.tsv\r\n",
      "------------------------------\r\n",
      "raw dialog:\r\n",
      "2008-09-03T06:57:00.000Z\tbrettley\t\tif i installed another linux opperating system next to ubuntu, would it take the GRUB i have now and modify it?\r\n",
      "2008-09-03T06:59:00.000Z\twols_\tbrettley\tit would install anotehr bootloader\r\n",
      "2008-09-03T07:00:00.000Z\tbrettley\twols_\tlike what?\r\n",
      "\r\n",
      "\r\n",
      "------------------------------\r\n",
      "keywords:\r\n",
      "['installed', 'linux', 'system', 'ubuntu', 'install', 'bootloader']\r\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\r\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\r\n",
      "file:\t./test/100003.tsv\r\n",
      "------------------------------\r\n",
      "raw dialog:\r\n",
      "2009-02-08T09:02:00.000Z\tkapace_laptop\t\thello, my friend is using live-cd and would like me to help him partition his drive\r\n",
      "2009-02-08T09:03:00.000Z\tkapace_laptop\t\tis there a way to do this remotely using remote access?\r\n",
      "2009-02-08T09:03:00.000Z\tbrettley\tkapace_laptop\tyes it is\r\n",
      "\r\n",
      "\r\n",
      "------------------------------\r\n",
      "keywords:\r\n",
      "['partition', 'drive']\r\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\r\n"
     ]
    }
   ],
   "source": [
    "!python3 predict_naive.py ./test/100001.tsv ./test/100002.tsv ./test/100003.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looks reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And let's train a LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load LDA.py\n",
    "\n",
    "'''\n",
    "\n",
    "@author: ZiqiLiu\n",
    "\n",
    "\n",
    "@file: LDA.py\n",
    "\n",
    "@time: 2017/11/11 下午1:34\n",
    "\n",
    "@desc:\n",
    "'''\n",
    "\n",
    "from gensim import corpora, models\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('loading data...')\n",
    "with codecs.open('whole_dialogs_stem', 'r', 'utf-8') as f:\n",
    "    raw_corpus = [line.split() for line in f]\n",
    "print(\"%d dialogs loaded.\" % len(raw_corpus))\n",
    "\n",
    "print('begin training...')\n",
    "dictionary = corpora.Dictionary(raw_corpus)\n",
    "dictionary.filter_extremes(keep_n=200000)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in raw_corpus]\n",
    "model = models.LdaMulticore(corpus, id2word=dictionary, workers=3,\n",
    "                            num_topics=50)\n",
    "\n",
    "dictionary.save('ubuntu.dict')\n",
    "model.save('ubuntu.lda')\n",
    "\n",
    "print(\n",
    "    'training finished. Model saved in ubuntu.lda. Dictionary saved in ubuntu.dic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, let's see how LDA perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "file:\t./test/100001.tsv\n",
      "--------------------------------------------------\n",
      "raw dialog:\n",
      "2008-09-02T04:02:00.000Z\tchuck\tbrettley\tHAI THAR\n",
      "2008-09-02T04:04:00.000Z\tbrettley\tchuck\tgo awai\n",
      "2008-09-02T04:10:00.000Z\tchuck\tbrettley\thello\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LDA predict:\n",
      "\n",
      "potential topic\n",
      "[(15, 0.50454097205436121), (24, 0.25545902794563885)]\n",
      "\n",
      "potential keywords:\n",
      "0.216*\"ubuntu\" + 0.051*\"debian\" + 0.043*\"use\" + 0.038*\"distro\" + 0.026*\"support\" + 0.022*\"base\" + 0.017*\"develop\" + 0.015*\"gentoo\" + 0.013*\"fedora\" + 0.011*\"like\"\n",
      "\n",
      "filtered keywords:\n",
      "set()\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "file:\t./test/100002.tsv\n",
      "--------------------------------------------------\n",
      "raw dialog:\n",
      "2008-09-03T06:57:00.000Z\tbrettley\t\tif i installed another linux opperating system next to ubuntu, would it take the GRUB i have now and modify it?\n",
      "2008-09-03T06:59:00.000Z\twols_\tbrettley\tit would install anotehr bootloader\n",
      "2008-09-03T07:00:00.000Z\tbrettley\twols_\tlike what?\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LDA predict:\n",
      "\n",
      "potential topic\n",
      "[(2, 0.43541507908375887), (15, 0.14665299826033945), (5, 0.11372750213271068), (12, 0.098093812579024983), (46, 0.07665138512987206), (49, 0.070792556147628485)]\n",
      "\n",
      "potential keywords:\n",
      "0.092*\"boot\" + 0.056*\"instal\" + 0.045*\"ubuntu\" + 0.040*\"grub\" + 0.032*\"window\" + 0.024*\"live\" + 0.021*\"can\" + 0.017*\"tri\" + 0.014*\"get\" + 0.013*\"just\"\n",
      "\n",
      "filtered keywords:\n",
      "{'instal', 'grub', 'ubuntu'}\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "file:\t./test/100003.tsv\n",
      "--------------------------------------------------\n",
      "raw dialog:\n",
      "2009-02-08T09:02:00.000Z\tkapace_laptop\t\thello, my friend is using live-cd and would like me to help him partition his drive\n",
      "2009-02-08T09:03:00.000Z\tkapace_laptop\t\tis there a way to do this remotely using remote access?\n",
      "2009-02-08T09:03:00.000Z\tbrettley\tkapace_laptop\tyes it is\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LDA predict:\n",
      "\n",
      "potential topic\n",
      "[(42, 0.27349562759684076), (16, 0.23013982104922226), (35, 0.20614877586447525), (2, 0.14522361257726588), (37, 0.084992162912196634)]\n",
      "\n",
      "potential keywords:\n",
      "0.099*\"drive\" + 0.038*\"hard\" + 0.031*\"disk\" + 0.020*\"data\" + 0.018*\"backup\" + 0.015*\"can\" + 0.014*\"hdd\" + 0.014*\"extern\" + 0.013*\"just\" + 0.012*\"raid\"\n",
      "\n",
      "filtered keywords:\n",
      "{'drive'}\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "!python3 predict_LDA.py ./test/100001.tsv ./test/100002.tsv ./test/100003.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the extracted keywords is given by \"filtered keywords\"\n",
    "\n",
    "Seems reasonable.\n",
    "\n",
    "### some discussion here:\n",
    "\n",
    "1. First test case give null keyword, which is reasonable, as that dialog is meaningless.  But if we don't filter the keywords (i.e, only keep those keywords appear in topic words), the first test case, which only few meaningless keyword, will give strange results, as its still forced classified into some category, which the dialog is meaningless at all.\n",
    "\n",
    "2. Actually I tried to train the LDA model based on un-stemming corpus, which totally mess up. Prove that saysing: garbage in garbage out. Stemming is a critical preprocessing to construct a clear word vector space, and it's critical to those models based on word vector space and statistic&probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "\n",
    "1. In the experiment, the naive method can give reasonable keyword regarding high frequency topic.\n",
    "Potential imporvement: bigram and trigram\n",
    "\n",
    "2. But the LDA also give reasonable result, but it sometime mess up with part of the test case. Aside from the discussion above, I still have some further thoughts: \n",
    "    * need further preprocessing. In ubuntu dialog corpus, there're many file path, URL, numbers, either filtered these out, or mapping them to specific word vector.\n",
    "    * need parameter tuning. Due to the time and computing resource limit I haven't tune the paramter very well, just pick reasonable and conservative number.\n",
    "    * The dialog is quite short, with only few word that can represent the feature of the topic. I have a project using LDA for topic classification of newspaper articles, which make much sense than ubuntu corpus. I suspect whether LDA is a good model for such a corpus.There're still many statistic and probability based model to try.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
